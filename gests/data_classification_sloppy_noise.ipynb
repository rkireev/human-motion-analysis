{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Steps:\n",
    "* Reinterpolete the datasets in order to all observations to have the same length (20 timestamps)\n",
    "* Roughly estimate parameters of classifiers with both datasets separately and together\n",
    "* Compute first integral and add it to datasets\n",
    "* Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inter(Obs, ndots=100):\n",
    "    interpolated = []\n",
    "    for obs in Obs:\n",
    "        x = np.linspace(0, 100, num=obs.shape[0], endpoint=True)\n",
    "        f = interp1d(x, obs, kind='slinear')\n",
    "        xnew = np.linspace(0, 100, num=ndots, endpoint=True)\n",
    "        interpolated.append(f(xnew))\n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def integrate(data):\n",
    "    data_int = deepcopy(data)\n",
    "    for block in data_int:\n",
    "        for signal_type in data_int[block]:\n",
    "            for axis in data_int[block][signal_type]:\n",
    "                for obs_ind in range(len(data_int[block][signal_type][axis])):\n",
    "                    data_int[block][signal_type][axis][obs_ind] = np.cumsum(data_int[block][signal_type][axis][obs_ind])\n",
    "    return data_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = {}\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in reader:\n",
    "            block = row[-1][1:]\n",
    "            if block.isdigit():\n",
    "                block = int(block)\n",
    "            del row[-1]\n",
    "            del row[0]\n",
    "            if not block in data:\n",
    "                data[block] = {}\n",
    "                data[block]['gyr'] = {}\n",
    "                data[block]['acc'] = {}\n",
    "                data[block]['gyr']['x'] = []\n",
    "                data[block]['gyr']['y'] = []\n",
    "                data[block]['gyr']['z'] = []\n",
    "                data[block]['acc']['x'] = []\n",
    "                data[block]['acc']['y'] = []\n",
    "                data[block]['acc']['z'] = []\n",
    "            frow = np.array(map(float, row))\n",
    "            data[block]['acc']['x'].append(np.array(frow[range(0,len(row),6)]))\n",
    "            data[block]['acc']['y'].append(np.array(frow[range(1,len(row),6)]))\n",
    "            data[block]['acc']['z'].append(np.array(frow[range(2,len(row),6)]))\n",
    "            data[block]['gyr']['x'].append(np.array(frow[range(3,len(row),6)]))\n",
    "            data[block]['gyr']['y'].append(np.array(frow[range(4,len(row),6)]))\n",
    "            data[block]['gyr']['z'].append(np.array(frow[range(5,len(row),6)]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_from_dict_interpoleted(data, ndots=20, use_gyro=True):\n",
    "    columns = ['block']\n",
    "    if use_gyro:\n",
    "        for signal_type in ['acc', 'gyr']:\n",
    "            for axis in ['x', 'y', 'z']:\n",
    "                for ind in range(ndots):\n",
    "                    columns.append('_'.join((signal_type, axis, str(ind))))\n",
    "    else:\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            for ind in range(ndots):\n",
    "                columns.append('_'.join(('acc', axis, str(ind))))\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for block in data:\n",
    "        acc_x = inter(data[block]['acc']['x'], ndots)\n",
    "        acc_y = inter(data[block]['acc']['y'], ndots)\n",
    "        acc_z = inter(data[block]['acc']['z'], ndots)\n",
    "        if use_gyro:\n",
    "            gyr_x = inter(data[block]['gyr']['x'], ndots)\n",
    "            gyr_y = inter(data[block]['gyr']['y'], ndots)\n",
    "            gyr_z = inter(data[block]['gyr']['z'], ndots)\n",
    "        #print np.hstack(([[block]]*len(acc_x), acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z))\n",
    "            df = df.append(pd.DataFrame(np.hstack(([[block]]*len(acc_x), acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z)), columns=columns))\n",
    "        else:\n",
    "            df = df.append(pd.DataFrame(np.hstack(([[block]]*len(acc_x), acc_x, acc_y, acc_z)), columns=columns))\n",
    "    df.reset_index(inplace=True)\n",
    "    del df['index']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(data, var, n):\n",
    "    data_noise = deepcopy(data)\n",
    "    for block in data:\n",
    "        for ind in np.random.choice(range(len(data[block]['acc']['x'])), n):\n",
    "            obs = np.vstack((data[block]['acc']['x'][ind], data[block]['acc']['y'][ind], \n",
    "                             data[block]['acc']['z'][ind], data[block]['gyr']['x'][ind], \n",
    "                             data[block]['gyr']['y'][ind], data[block]['gyr']['z'][ind]))\n",
    "            obs = obs + np.transpose(np.random.multivariate_normal(np.zeros(6), \n",
    "                                                                   var*np.identity(6), \n",
    "                                                                   len(data[block]['gyr']['z'][ind])))\n",
    "            data_noise[block]['acc']['x'].append(obs[0])\n",
    "            data_noise[block]['acc']['y'].append(obs[1])\n",
    "            data_noise[block]['acc']['z'].append(obs[2])\n",
    "            data_noise[block]['gyr']['x'].append(obs[3])\n",
    "            data_noise[block]['gyr']['y'].append(obs[4])\n",
    "            data_noise[block]['gyr']['z'].append(obs[5])\n",
    "            \n",
    "    return data_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = read_data('data1.csv')  # Your data\n",
    "data2 = read_data('data2.csv')  # My data (I dont have a gyroscope)\n",
    "data1 = add_noise(data1, 0.5, 100)  # Your data\n",
    "data2 = add_noise(data2, 0.5, 100)  # My data (I dont have a gyroscope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df_from_dict_interpoleted(data1)\n",
    "df2 = df_from_dict_interpoleted(data2)\n",
    "df1_acc = df_from_dict_interpoleted(data1, use_gyro=False)\n",
    "df2_acc = df_from_dict_interpoleted(data2, use_gyro=False)\n",
    "df1_vel = df_from_dict_interpoleted(integrate(data1), use_gyro=False)\n",
    "df2_vel = df_from_dict_interpoleted(integrate(data2), use_gyro=False)\n",
    "df1i = pd.concat([df1, df_from_dict_interpoleted(integrate(data1))], axis=1)\n",
    "df2i = pd.concat([df2, df_from_dict_interpoleted(integrate(data2))], axis=1)\n",
    "df = df1.append(df2).reset_index()\n",
    "dfi = df1i.append(df2i).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block</th>\n",
       "      <th>acc_x_0</th>\n",
       "      <th>acc_x_1</th>\n",
       "      <th>acc_y_0</th>\n",
       "      <th>acc_y_1</th>\n",
       "      <th>acc_z_0</th>\n",
       "      <th>acc_z_1</th>\n",
       "      <th>gyr_x_0</th>\n",
       "      <th>gyr_x_1</th>\n",
       "      <th>gyr_y_0</th>\n",
       "      <th>gyr_y_1</th>\n",
       "      <th>gyr_z_0</th>\n",
       "      <th>gyr_z_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.443272</td>\n",
       "      <td>1.867550</td>\n",
       "      <td>-0.516200</td>\n",
       "      <td>0.711314</td>\n",
       "      <td>-0.423058</td>\n",
       "      <td>0.224063</td>\n",
       "      <td>0.853332</td>\n",
       "      <td>0.242843</td>\n",
       "      <td>0.681804</td>\n",
       "      <td>-0.717361</td>\n",
       "      <td>0.966665</td>\n",
       "      <td>-0.802818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.019177</td>\n",
       "      <td>-1.126338</td>\n",
       "      <td>1.507303</td>\n",
       "      <td>-0.447238</td>\n",
       "      <td>-0.211618</td>\n",
       "      <td>2.161715</td>\n",
       "      <td>0.022056</td>\n",
       "      <td>0.107672</td>\n",
       "      <td>1.246061</td>\n",
       "      <td>-0.847301</td>\n",
       "      <td>0.539355</td>\n",
       "      <td>-1.014824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.040702</td>\n",
       "      <td>-0.792967</td>\n",
       "      <td>0.478340</td>\n",
       "      <td>-0.856840</td>\n",
       "      <td>0.180160</td>\n",
       "      <td>1.139428</td>\n",
       "      <td>-0.246832</td>\n",
       "      <td>-0.145443</td>\n",
       "      <td>0.483045</td>\n",
       "      <td>-0.688180</td>\n",
       "      <td>-1.123851</td>\n",
       "      <td>0.047946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.005665</td>\n",
       "      <td>-0.802621</td>\n",
       "      <td>0.330418</td>\n",
       "      <td>0.065093</td>\n",
       "      <td>-1.042225</td>\n",
       "      <td>0.872419</td>\n",
       "      <td>0.415059</td>\n",
       "      <td>-0.176971</td>\n",
       "      <td>0.879244</td>\n",
       "      <td>0.361768</td>\n",
       "      <td>0.171608</td>\n",
       "      <td>0.317499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.764264</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>0.059651</td>\n",
       "      <td>-1.656166</td>\n",
       "      <td>0.181832</td>\n",
       "      <td>0.463823</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>-0.386293</td>\n",
       "      <td>0.408607</td>\n",
       "      <td>1.217196</td>\n",
       "      <td>-0.454792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      block   acc_x_0   acc_x_1   acc_y_0   acc_y_1   acc_z_0   acc_z_1  \\\n",
       "1546   15.0 -0.443272  1.867550 -0.516200  0.711314 -0.423058  0.224063   \n",
       "991    10.0  1.019177 -1.126338  1.507303 -0.447238 -0.211618  2.161715   \n",
       "84      1.0 -0.040702 -0.792967  0.478340 -0.856840  0.180160  1.139428   \n",
       "675     7.0 -0.005665 -0.802621  0.330418  0.065093 -1.042225  0.872419   \n",
       "524     5.0  0.764264  0.114397 -0.583690  0.059651 -1.656166  0.181832   \n",
       "\n",
       "       gyr_x_0   gyr_x_1   gyr_y_0   gyr_y_1   gyr_z_0   gyr_z_1  \n",
       "1546  0.853332  0.242843  0.681804 -0.717361  0.966665 -0.802818  \n",
       "991   0.022056  0.107672  1.246061 -0.847301  0.539355 -1.014824  \n",
       "84   -0.246832 -0.145443  0.483045 -0.688180 -1.123851  0.047946  \n",
       "675   0.415059 -0.176971  0.879244  0.361768  0.171608  0.317499  \n",
       "524   0.463823  0.023914 -0.386293  0.408607  1.217196 -0.454792  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_dict_interpoleted(data1,  ndots=2).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target1 = df1['block']\n",
    "del df1['block']\n",
    "X1 = df1\n",
    "target2 = df2['block']\n",
    "del df2['block']\n",
    "X2 = df2\n",
    "target = df['block']\n",
    "del df['block']\n",
    "del df['index']\n",
    "X = df\n",
    "target1i = df1i['block'].ix[:,0]\n",
    "del df1i['block']\n",
    "X1i = df1i.as_matrix()\n",
    "target2i = df2i['block'].ix[:,0]\n",
    "del df2i['block']\n",
    "X2i = df2i.as_matrix()\n",
    "targeti = dfi['block'].ix[:,0]\n",
    "del dfi['block']\n",
    "del dfi['index']\n",
    "Xi = dfi.as_matrix()\n",
    "target1_acc = df1_acc['block']\n",
    "del df1_acc['block']\n",
    "X1_acc = df1_acc\n",
    "target2_acc = df2_acc['block']\n",
    "del df2_acc['block']\n",
    "X2_acc = df2_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target1_vel = df1_vel['block']\n",
    "del df1_vel['block']\n",
    "X1_vel = df1_vel\n",
    "target2_vel = df2_vel['block']\n",
    "del df2_vel['block']\n",
    "X2_vel = df2_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have:\n",
    "\n",
    "X1 - first dataset\n",
    "\n",
    "X2 - second dataset\n",
    "\n",
    "X - both datasets \n",
    "\n",
    "X1i - first dataset + its integrated observations\n",
    "\n",
    "X2i - second dataset + its integrated observations\n",
    "\n",
    "Xi - both datasets + its integrated observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sloppy estimate parameters of XGB and SVM clsfrs. Also we consider a dummy estimator, based on class frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_xgb():\n",
    "    max_depth=np.random.choice([5,10,15,20],1)\n",
    "    learning_rate=np.random.choice([0.01, 0.05, 0.1, 0.5, 1],1)\n",
    "    n_estimators=np.random.choice([10, 50, 100, 150, 200, 300],1)\n",
    "    subsample=np.random.choice([0.3, 0.6, 0.7, 0.8, 1],1)\n",
    "    colsample_bytree=np.random.choice([0.4, 0.6, 0.7, 0.8, 1],1)\n",
    "    return XGBClassifier(learning_rate=learning_rate[0], n_estimators=n_estimators[0], max_depth=max_depth[0],\n",
    "                        subsample=subsample[0], colsample_bytree=colsample_bytree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    xgbc = params_xgb()\n",
    "    scores_xgb = cross_validation.cross_val_score(xgbc, Xi, targeti, cv=4)\n",
    "    if np.mean(scores_xgb)>0.6:\n",
    "        print xgbc\n",
    "        print np.mean(scores_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-125249e9fa20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n\u001b[0;32m      2\u001b[0m               {'C': [1, 10, 100, 1000], 'kernel': ['rbf']}]\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgrid_search1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgrid_search1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargeti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    }
   ],
   "source": [
    "param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf']}]\n",
    "grid_search1 = grid_search.GridSearchCV(svc, param_grid=param_grid)\n",
    "grid_search1.fit(Xi, targeti)\n",
    "print(grid_search1.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def svm_xgb_dummy(X, y):\n",
    "    dumb = DummyClassifier()\n",
    "    svc = SVC(kernel='linear')\n",
    "    xgb = XGBClassifier(colsample_bytree=0.4, learning_rate=0.05, max_depth=10, n_estimators=200, subsample=0.6)\n",
    "    scores_svc = cross_validation.cross_val_score(svc, X, y, cv=4)\n",
    "    scores_dumb = cross_validation.cross_val_score(dumb, X, y, cv=4)\n",
    "    scores_xgb = cross_validation.cross_val_score(xgb, X, y, cv=4)\n",
    "    return np.mean(scores_svc), np.mean(scores_xgb), np.mean(scores_dumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['dataset'] = []\n",
    "results['SVM'] = []\n",
    "results['XGB'] = []\n",
    "results['Dummy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X1, target1)\n",
    "results['dataset'].append('data1')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X2, target2)\n",
    "results['dataset'].append('data2')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X, target)\n",
    "results['dataset'].append('data1+data2')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X1i, target1i)\n",
    "results['dataset'].append('data1+integrated')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X2i, target2i)\n",
    "results['dataset'].append('data2+integrated')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(Xi, targeti)\n",
    "results['dataset'].append('data1+data2+integrated')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X1_acc, target1_acc)\n",
    "results['dataset'].append('data1 accelerometer only')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X2_acc, target2_acc)\n",
    "results['dataset'].append('data2 accelerometer only')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X1_vel, target1_vel)\n",
    "results['dataset'].append('data1 velocity only')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)\n",
    "s_svm, s_xgb, s_dum = svm_xgb_dummy(X2_vel, target2_vel)\n",
    "results['dataset'].append('data2 velocity only')\n",
    "results['SVM'].append(s_svm)\n",
    "results['XGB'].append(s_xgb)\n",
    "results['Dummy'].append(s_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy</th>\n",
       "      <th>SVM</th>\n",
       "      <th>XGB</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998849</td>\n",
       "      <td>data1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059539</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.927311</td>\n",
       "      <td>data2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065671</td>\n",
       "      <td>0.909954</td>\n",
       "      <td>0.922761</td>\n",
       "      <td>data1+data2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.069601</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.990115</td>\n",
       "      <td>data1+integrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063005</td>\n",
       "      <td>0.957942</td>\n",
       "      <td>0.924485</td>\n",
       "      <td>data2+integrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.063098</td>\n",
       "      <td>0.941025</td>\n",
       "      <td>0.924204</td>\n",
       "      <td>data1+data2+integrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.057535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>data1 accelerometer only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.058484</td>\n",
       "      <td>0.933546</td>\n",
       "      <td>0.932397</td>\n",
       "      <td>data2 accelerometer only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.998258</td>\n",
       "      <td>0.981452</td>\n",
       "      <td>data1 velocity only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.055618</td>\n",
       "      <td>0.959741</td>\n",
       "      <td>0.888198</td>\n",
       "      <td>data2 velocity only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dummy       SVM       XGB                   dataset\n",
       "0  0.055781  1.000000  0.998849                     data1\n",
       "1  0.059539  0.936416  0.927311                     data2\n",
       "2  0.065671  0.909954  0.922761               data1+data2\n",
       "3  0.069601  0.999429  0.990115          data1+integrated\n",
       "4  0.063005  0.957942  0.924485          data2+integrated\n",
       "5  0.063098  0.941025  0.924204    data1+data2+integrated\n",
       "6  0.057535  1.000000  0.998266  data1 accelerometer only\n",
       "7  0.058484  0.933546  0.932397  data2 accelerometer only\n",
       "8  0.064391  0.998258  0.981452       data1 velocity only\n",
       "9  0.055618  0.959741  0.888198       data2 velocity only"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
